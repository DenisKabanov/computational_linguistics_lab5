{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "import warnings # выключение отображение warning-ов\n",
    "import transformers\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'topics'\n",
    "topic_names = ['text']\n",
    "for x in os.listdir(dataset_path):\n",
    "    topic_names.append(x)\n",
    "# topic_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>alone</th>\n",
       "      <th>america</th>\n",
       "      <th>angel</th>\n",
       "      <th>anger</th>\n",
       "      <th>animal</th>\n",
       "      <th>baby</th>\n",
       "      <th>beach</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>beauty</th>\n",
       "      <th>...</th>\n",
       "      <th>trust</th>\n",
       "      <th>truth</th>\n",
       "      <th>war</th>\n",
       "      <th>warning</th>\n",
       "      <th>water</th>\n",
       "      <th>weather</th>\n",
       "      <th>wedding</th>\n",
       "      <th>winter</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 145 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, alone, america, angel, anger, animal, baby, beach, beautiful, beauty, believe, birth, brother, butterfly, car, carpe diem, change, chicago, childhood, children, christmas, cinderella, city, courage, crazy, culture, dance, dark, daughter, death, depression, despair, destiny, dream, evil, faith, family, father, fear, fire, food, football, freedom, friend, frog, funeral, funny, future, girl, god, graduation, greed, green, hair, happiness, happy, hate, heaven, hero, home, hope, house, hunting, husband, identity, innocence, january, joy, june, justice, kiss, laughter, life, lonely, loss, lost, love, lust, marriage, memory, mirror, money, moon, mother, murder, music, nature, night, ocean, paris, passion, peace, pink, poem, poetry, poverty, power, racism, rain, rainbow, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 145 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.DataFrame(columns=topic_names)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [2:31:58<00:00, 63.32s/it] \n"
     ]
    }
   ],
   "source": [
    "for topic in tqdm(os.listdir(dataset_path)):\n",
    "    for file in os.listdir(f'{dataset_path}/{topic}'):\n",
    "        with open(f'{dataset_path}/{topic}/{file}', encoding = 'utf-8') as f:\n",
    "            str = f.read()\n",
    "            index = dataset.index[dataset['text'] == str]\n",
    "            # print(index.size)\n",
    "            if index.size != 0:  #  если текст есть в датасете\n",
    "                # print('not None')\n",
    "                dataset[topic][index] = 1\n",
    "            else:   # если текст отсутствует в датасете\n",
    "                \n",
    "                dataset = dataset.append({'text': str, topic: 1}, ignore_index=True)\n",
    "                # print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>alone</th>\n",
       "      <th>america</th>\n",
       "      <th>angel</th>\n",
       "      <th>anger</th>\n",
       "      <th>animal</th>\n",
       "      <th>baby</th>\n",
       "      <th>beach</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>beauty</th>\n",
       "      <th>...</th>\n",
       "      <th>trust</th>\n",
       "      <th>truth</th>\n",
       "      <th>war</th>\n",
       "      <th>warning</th>\n",
       "      <th>water</th>\n",
       "      <th>weather</th>\n",
       "      <th>wedding</th>\n",
       "      <th>winter</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loneliness\\nhis childhood\\nwarms him up\\nlonel...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sit in a chair\\nalone\\nPeople I thought were\\n...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tell me why it has to be this way\\nwhy I must ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A debtor to mercy alone, of covenant mercy I s...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A year has been passed since I left home\\nAgai...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13732</th>\n",
       "      <td>Wake not for the world-heard thunder,\\nNor the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13733</th>\n",
       "      <td>We may roam through this world, like a child a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13734</th>\n",
       "      <td>When you were born in this world\\nEveryone lau...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13735</th>\n",
       "      <td>WORLD, take good notice, silver stars fading,\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13736</th>\n",
       "      <td>World was in the face of the beloved--,\\nbut s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13737 rows × 145 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text alone america angel  \\\n",
       "0      loneliness\\nhis childhood\\nwarms him up\\nlonel...     1     NaN   NaN   \n",
       "1      Sit in a chair\\nalone\\nPeople I thought were\\n...     1     NaN   NaN   \n",
       "2      Tell me why it has to be this way\\nwhy I must ...     1     NaN   NaN   \n",
       "3      A debtor to mercy alone, of covenant mercy I s...     1     NaN   NaN   \n",
       "4      A year has been passed since I left home\\nAgai...     1     NaN   NaN   \n",
       "...                                                  ...   ...     ...   ...   \n",
       "13732  Wake not for the world-heard thunder,\\nNor the...   NaN     NaN   NaN   \n",
       "13733  We may roam through this world, like a child a...   NaN     NaN   NaN   \n",
       "13734  When you were born in this world\\nEveryone lau...   NaN     NaN   NaN   \n",
       "13735  WORLD, take good notice, silver stars fading,\\...   NaN     NaN   NaN   \n",
       "13736  World was in the face of the beloved--,\\nbut s...   NaN     NaN   NaN   \n",
       "\n",
       "      anger animal baby beach beautiful beauty  ... trust truth  war warning  \\\n",
       "0       NaN    NaN  NaN   NaN       NaN    NaN  ...   NaN   NaN  NaN     NaN   \n",
       "1       NaN    NaN  NaN   NaN       NaN    NaN  ...   NaN   NaN  NaN     NaN   \n",
       "2       NaN    NaN  NaN   NaN       NaN    NaN  ...   NaN   NaN  NaN     NaN   \n",
       "3       NaN    NaN  NaN   NaN       NaN    NaN  ...   NaN   NaN  NaN     NaN   \n",
       "4       NaN    NaN  NaN   NaN       NaN    NaN  ...   NaN   NaN  NaN     NaN   \n",
       "...     ...    ...  ...   ...       ...    ...  ...   ...   ...  ...     ...   \n",
       "13732   NaN    NaN  NaN   NaN       NaN    NaN  ...   NaN   NaN  NaN     NaN   \n",
       "13733   NaN    NaN  NaN   NaN       NaN    NaN  ...   NaN   NaN  NaN     NaN   \n",
       "13734   NaN    NaN  NaN   NaN       NaN    NaN  ...   NaN   NaN  NaN     NaN   \n",
       "13735   NaN    NaN  NaN   NaN       NaN    NaN  ...   NaN   NaN  NaN     NaN   \n",
       "13736   NaN    NaN  NaN   NaN       NaN    NaN  ...   NaN   NaN  NaN     NaN   \n",
       "\n",
       "      water weather wedding winter work world  \n",
       "0       NaN     NaN     NaN    NaN  NaN   NaN  \n",
       "1       NaN     NaN     NaN    NaN  NaN   NaN  \n",
       "2       NaN     NaN     NaN    NaN  NaN   NaN  \n",
       "3       NaN     NaN     NaN    NaN  NaN   NaN  \n",
       "4       NaN     NaN     NaN    NaN  NaN   NaN  \n",
       "...     ...     ...     ...    ...  ...   ...  \n",
       "13732   NaN     NaN     NaN    NaN  NaN     1  \n",
       "13733   NaN     NaN     NaN    NaN  NaN     1  \n",
       "13734   NaN     NaN     NaN    NaN  NaN     1  \n",
       "13735   NaN     NaN     NaN    NaN  NaN     1  \n",
       "13736   NaN     NaN     NaN    NaN  NaN     1  \n",
       "\n",
       "[13737 rows x 145 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fillna = dataset.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>passion</th>\n",
       "      <th>daughter</th>\n",
       "      <th>weather</th>\n",
       "      <th>animal</th>\n",
       "      <th>sea</th>\n",
       "      <th>innocence</th>\n",
       "      <th>change</th>\n",
       "      <th>angel</th>\n",
       "      <th>brother</th>\n",
       "      <th>...</th>\n",
       "      <th>freedom</th>\n",
       "      <th>dark</th>\n",
       "      <th>hate</th>\n",
       "      <th>music</th>\n",
       "      <th>warning</th>\n",
       "      <th>teacher</th>\n",
       "      <th>peace</th>\n",
       "      <th>chicago</th>\n",
       "      <th>butterfly</th>\n",
       "      <th>murder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Your passion is permanently etched on my soul,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When man had ceased to utter his lament, A god...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O Gentle Love, ungentle for thy deed, Thou mak...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PASSION TOUCHED HER LIPS Passion touched her l...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You eclipse me &amp; I have stained the Sun with b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13731</th>\n",
       "      <td>Violence is nobody s leaven. Murder will not g...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13732</th>\n",
       "      <td>St. Patrick s Church was sparsely filled For F...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13733</th>\n",
       "      <td>No one sleeps more beautifully than you. But I...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13734</th>\n",
       "      <td>Pastor in the pulpit, the Sanctuary, supposedl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13735</th>\n",
       "      <td>Murder the great sin That commits only by huma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13736 rows × 145 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  passion  daughter  \\\n",
       "0      Your passion is permanently etched on my soul,...        1         0   \n",
       "1      When man had ceased to utter his lament, A god...        1         0   \n",
       "2      O Gentle Love, ungentle for thy deed, Thou mak...        1         0   \n",
       "3      PASSION TOUCHED HER LIPS Passion touched her l...        1         0   \n",
       "4      You eclipse me & I have stained the Sun with b...        1         0   \n",
       "...                                                  ...      ...       ...   \n",
       "13731  Violence is nobody s leaven. Murder will not g...        0         0   \n",
       "13732  St. Patrick s Church was sparsely filled For F...        0         0   \n",
       "13733  No one sleeps more beautifully than you. But I...        0         0   \n",
       "13734  Pastor in the pulpit, the Sanctuary, supposedl...        0         0   \n",
       "13735  Murder the great sin That commits only by huma...        0         0   \n",
       "\n",
       "       weather  animal  sea  innocence  change  angel  brother  ...  freedom  \\\n",
       "0            0       0    0          0       0      0        0  ...        0   \n",
       "1            0       0    0          0       0      0        0  ...        0   \n",
       "2            0       0    0          0       0      0        0  ...        0   \n",
       "3            0       0    0          0       0      0        0  ...        0   \n",
       "4            0       0    0          0       0      0        0  ...        0   \n",
       "...        ...     ...  ...        ...     ...    ...      ...  ...      ...   \n",
       "13731        0       0    0          0       0      0        0  ...        0   \n",
       "13732        0       0    0          0       0      0        0  ...        0   \n",
       "13733        0       0    0          0       0      0        0  ...        0   \n",
       "13734        0       0    0          0       0      0        0  ...        0   \n",
       "13735        0       0    0          0       0      0        0  ...        0   \n",
       "\n",
       "       dark  hate  music  warning  teacher  peace  chicago  butterfly  murder  \n",
       "0         0     0      0        0        0      0        0          0       0  \n",
       "1         0     0      0        0        0      0        0          0       0  \n",
       "2         0     0      0        0        0      0        0          0       0  \n",
       "3         0     0      0        0        0      0        0          0       0  \n",
       "4         0     0      0        0        0      0        0          0       0  \n",
       "...     ...   ...    ...      ...      ...    ...      ...        ...     ...  \n",
       "13731     0     0      0        0        0      0        0          0       1  \n",
       "13732     0     0      0        0        0      0        0          0       1  \n",
       "13733     0     0      0        0        0      0        0          0       1  \n",
       "13734     0     0      0        0        0      0        0          0       1  \n",
       "13735     0     0      0        0        0      0        0          0       1  \n",
       "\n",
       "[13736 rows x 145 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'dataset_fillna.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset_fillna, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'dataset_fillna.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE_SYMBOLS_STR = u''.join(['№', '«', 'ђ', '°', '±', 'ћ', '‰', '»', 'ѓ', 'µ', '·', 'ґ', 'њ', 'ї', 'џ', 'є', '‹',\n",
    "                                '‡', '†', '¶', 'ќ', '€', '“', 'ў', '§', '„', '”', '\\ufeff', 'љ', '›', '•', '—', \n",
    "                                '\\x7f', '\\xad', '¤', '\\xa0', '\\u200e', 'Š', 'ō', 'ä', '™', '×', '\\'', '~'])\n",
    "\n",
    "regex_symb = re.compile('[%s]' % re.escape(EXCLUDE_SYMBOLS_STR))\n",
    "\n",
    "for row in range(dataset.shape[0]):\n",
    "    dataset[\"text\"][row] = regex_symb.sub(' ',  dataset[\"text\"][row]) # убираем дополнительные символы\n",
    "    dataset[\"text\"][row] = dataset[\"text\"][row].replace(\"\\n\", \" \") # убираем \\n\n",
    "    dataset[\"text\"][row] = re.sub(' +', ' ', dataset[\"text\"][row]) # оставляем максимум 1 пробел\n",
    "    dataset[\"text\"][row] = dataset[\"text\"][row].strip() # удаляем лишние пробелы в начале и конце статей\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'dataset_processed.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When man had ceased to utter his lament, A god then let me tell my tale of sorrow. WHAT hope of once more meeting is there now In the still-closed blossoms of this day? Both heaven and hell thrown open seest thou; What wav ring thoughts within the bosom play No longer doubt! Descending from the sky, She lifts thee in her arms to realms on high. And thus thou into Paradise wert brought, As worthy of a pure and endless life; Nothing was left, no wish, no hope, no thought, Here was the boundary of thine inmost strife: And seeing one so fair, so glorified, The fount of yearning tears was straightway dried. No motion stirr d the day s revolving wheel, In their own front the minutes seem d to go; The evening kiss, a true and binding seal, Ne er changing till the morrow s sunlight glow. The hours resembled sisters as they went. Yet each one from another different. The last hour s kiss, so sadly sweet, effac d A beauteous network of entwining love. Now on the threshold pause the feet, now haste. As though a flaming cherub bade them move; The unwilling eye the dark road wanders o er, Backward it looks, but closed it sees the door. And now within itself is closed this breast, As though it ne er were open, and as though, Vying with ev ry star, no moments blest Had, in its presence, felt a kindling glow; Sadness, reproach, repentance, weight of care, Hang heavy on it in the sultry air. Is not the world still left? The rocky steeps, Are they with holy shades no longer crown d? Grows not the harvest ripe? No longer creeps The espalier by the stream,--the copse around? Doth not the wondrous arch of heaven still rise, Now rich in shape, now shapeless to the eyes? As, seraph-like, from out the dark clouds chorus, With softness woven, graceful, light, and fair, Resembling Her, in the blue aether o er us, A slender figure hovers in the air,-- Thus didst thou see her joyously advance, The fairest of the fairest in the dance. Yet but a moment dost thou boldly dare To clasp an airy form instead of hers; Back to thine heart! thou lt find it better there, For there in changeful guise her image stirs What erst was one, to many turneth fast, In thousand forms, each dearer than the last. As at the door, on meeting lingerd she, And step by step my faithful ardour bless d, For the last kiss herself entreated me, And on my lips the last last kiss impress d,-- Thus clearly traced, the lov d one s form we view, With flames engraven on a heart so true,-- A heart that, firm as some embattled tower, Itself for her, her in itself reveres, For her rejoices in its lasting power, Conscious alone, when she herself appears; Feels itself freer in so sweet a thrall, And only beats to give her thanks in all. The power of loving, and all yearning sighs For love responsive were effaced and drown d; While longing hope for joyous enterprise Was form d, and rapid action straightway found; If love can e er a loving one inspire, Most lovingly it gave me now its fire; And twas through her!--an inward sorrow lay On soul and body, heavily oppress d; To mournful phantoms was my sight a prey, In the drear void of a sad tortured breast; Now on the well-known threshold Hope hath smil d, Herself appeareth in the sunlight mild. Unto the peace of God, which, as we read, Blesseth us more than reason e er bath done, Love s happy peace would I compare indeed, When in the presence of the dearest one. There rests the heart, and there that sweetest thought, The thought of being hers, is check d by nought. In the pure bosom doth a yearning float, Unto a holier, purer, unknown Being Its grateful aspiration to devote, The Ever-Nameless then unriddled seeing; We call it: piety!--such blest delight I feel a share in, when before her sight. Before her sight, as neath the sun s hot ray, Before her breath, as neath the spring s soft wind, In its deep wintry cavern melts away Self-love, so long in icy chains confin d; No selfishness and no self-will are nigh, For at her advent they were forced to fly. It seems as though she said: \"As hours pass by They spread before us life with kindly plan; Small knowledge did the yesterday supply, To know the morrow is conceal d from man; And if the thought of evening made me start, The sun at setting gladden d straight my heart. \"Act, then, as I, and look, with joyous mind, The moment in the face; nor linger thou! Meet it with speed, so fraught with life, so kind In action, and in love so radiant now; Let all things be where thou art, childlike ever, Thus thoult be all, thus, thou lt be vanquish d never.\" Thou speakest well, methought, for as thy guide The moment s favour did a god assign, And each one feels himself when by thy side, Fate s fav rite in a moment so divine; I tremble at thy look that bids me go, Why should I care such wisdom vast to know? Now am I far! And what would best befit The present minute? I could scarcely tell; Full many a rich possession offers it, These but offend, and I would fain repel. Yearnings unquenchable still drive me on, All counsel, save unbounded tears, is gone. Flow on, flow on in never-ceasing course, Yet may ye never quench my inward fire! Within my bosom heaves a mighty force, Where death and life contend in combat dire. Medicines may serve the body s pangs to still; Nought but the spirit fails in strength of will,-- Fails in conception; wherefore fails it so? A thousand times her image it portrays; Enchanting now, and now compell d to go, Now indistinct, now clothed in purest rays! How could the smallest comfort here be flowing? The ebb and flood, the coming and the going! Leave me here now, my life s companions true! Leave me alone on rock, in moor and heath; But courage! open lies the world to you, The glorious heavens above, the earth beneath; Observe, investigate, with searching eyes, And nature will disclose her mysteries. To me is all, I to myself am lost, Who the immortals fav rite erst was thought; They, tempting, sent Pandoras to my cost, So rich in wealth, with danger far more fraught; They urged me to those lips, with rapture crown d, Deserted me, and hurl d me to the ground.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"text\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passion</th>\n",
       "      <th>daughter</th>\n",
       "      <th>weather</th>\n",
       "      <th>animal</th>\n",
       "      <th>sea</th>\n",
       "      <th>innocence</th>\n",
       "      <th>change</th>\n",
       "      <th>angel</th>\n",
       "      <th>brother</th>\n",
       "      <th>world</th>\n",
       "      <th>...</th>\n",
       "      <th>freedom</th>\n",
       "      <th>dark</th>\n",
       "      <th>hate</th>\n",
       "      <th>music</th>\n",
       "      <th>warning</th>\n",
       "      <th>teacher</th>\n",
       "      <th>peace</th>\n",
       "      <th>chicago</th>\n",
       "      <th>butterfly</th>\n",
       "      <th>murder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13731</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13732</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13733</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13734</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13735</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13736 rows × 144 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       passion  daughter  weather  animal  sea  innocence  change  angel  \\\n",
       "0            1         0        0       0    0          0       0      0   \n",
       "1            1         0        0       0    0          0       0      0   \n",
       "2            1         0        0       0    0          0       0      0   \n",
       "3            1         0        0       0    0          0       0      0   \n",
       "4            1         0        0       0    0          0       0      0   \n",
       "...        ...       ...      ...     ...  ...        ...     ...    ...   \n",
       "13731        0         0        0       0    0          0       0      0   \n",
       "13732        0         0        0       0    0          0       0      0   \n",
       "13733        0         0        0       0    0          0       0      0   \n",
       "13734        0         0        0       0    0          0       0      0   \n",
       "13735        0         0        0       0    0          0       0      0   \n",
       "\n",
       "       brother  world  ...  freedom  dark  hate  music  warning  teacher  \\\n",
       "0            0      0  ...        0     0     0      0        0        0   \n",
       "1            0      0  ...        0     0     0      0        0        0   \n",
       "2            0      0  ...        0     0     0      0        0        0   \n",
       "3            0      0  ...        0     0     0      0        0        0   \n",
       "4            0      0  ...        0     0     0      0        0        0   \n",
       "...        ...    ...  ...      ...   ...   ...    ...      ...      ...   \n",
       "13731        0      0  ...        0     0     0      0        0        0   \n",
       "13732        0      0  ...        0     0     0      0        0        0   \n",
       "13733        0      0  ...        0     0     0      0        0        0   \n",
       "13734        0      0  ...        0     0     0      0        0        0   \n",
       "13735        0      0  ...        0     0     0      0        0        0   \n",
       "\n",
       "       peace  chicago  butterfly  murder  \n",
       "0          0        0          0       0  \n",
       "1          0        0          0       0  \n",
       "2          0        0          0       0  \n",
       "3          0        0          0       0  \n",
       "4          0        0          0       0  \n",
       "...      ...      ...        ...     ...  \n",
       "13731      0        0          0       1  \n",
       "13732      0        0          0       1  \n",
       "13733      0        0          0       1  \n",
       "13734      0        0          0       1  \n",
       "13735      0        0          0       1  \n",
       "\n",
       "[13736 rows x 144 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.drop(\"text\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделение на датасет для тренировки и проверки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['passion', 'daughter', 'weather', 'animal', 'sea', 'innocence',\n",
       "       'change', 'angel', 'brother', 'world', 'paris', 'work', 'soldier',\n",
       "       'together', 'christmas', 'dance', 'sleep', 'happy', 'faith',\n",
       "       'school', 'courage', 'loss', 'romance', 'nature', 'identity',\n",
       "       'car', 'greed', 'lost', 'june', 'truth', 'destiny', 'trust',\n",
       "       'children', 'believe', 'culture', 'silver', 'frog', 'red', 'food',\n",
       "       'sun', 'romantic', 'time', 'lust', 'mother', 'heaven', 'memory',\n",
       "       'crazy', 'money', 'racism', 'girl', 'home', 'evil', 'future',\n",
       "       'running', 'carpe diem', 'sorrow', 'january', 'funeral', 'travel',\n",
       "       'spring', 'fire', 'poem', 'suicide', 'remember', 'poetry',\n",
       "       'football', 'green', 'laughter', 'cinderella', 'poverty', 'today',\n",
       "       'birth', 'happiness', 'life', 'son', 'winter', 'hair', 'rose',\n",
       "       'rainbow', 'god', 'star', 'house', 'power', 'sympathy',\n",
       "       'depression', 'snake', 'night', 'sick', 'despair', 'husband',\n",
       "       'rain', 'love', 'river', 'city', 'childhood', 'sister', 'war',\n",
       "       'kiss', 'beach', 'marriage', 'sometimes', 'father', 'joy',\n",
       "       'success', 'hero', 'respect', 'mirror', 'justice', 'lonely',\n",
       "       'dream', 'song', 'hunting', 'funny', 'pink', 'anger', 'hope',\n",
       "       'friend', 'beautiful', 'america', 'death', 'wedding', 'alone',\n",
       "       'graduation', 'baby', 'water', 'fear', 'moon', 'beauty', 'sky',\n",
       "       'ocean', 'family', 'thanks', 'swimming', 'summer', 'freedom',\n",
       "       'dark', 'hate', 'music', 'warning', 'teacher', 'peace', 'chicago',\n",
       "       'butterfly', 'murder'], dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dataset.columns[1:].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") # create a tokenizer that corresponds to BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "    # take a batch of texts\n",
    "    text = examples[\"text\"]\n",
    "    # encode them\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=256)\n",
    "    # add labels\n",
    "    labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "    # create numpy array of shape (batch_size, num_labels)\n",
    "    labels_matrix = np.zeros((len(text), len(labels)))\n",
    "    # fill numpy array\n",
    "    for idx, label in enumerate(labels):\n",
    "        labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "    encoding[\"labels\"] = labels_matrix.tolist()\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'passion', 'daughter', 'weather', 'animal', 'sea', 'innocence', 'change', 'angel', 'brother', 'world', 'paris', 'work', 'soldier', 'together', 'christmas', 'dance', 'sleep', 'happy', 'faith', 'school', 'courage', 'loss', 'romance', 'nature', 'identity', 'car', 'greed', 'lost', 'june', 'truth', 'destiny', 'trust', 'children', 'believe', 'culture', 'silver', 'frog', 'red', 'food', 'sun', 'romantic', 'time', 'lust', 'mother', 'heaven', 'memory', 'crazy', 'money', 'racism', 'girl', 'home', 'evil', 'future', 'running', 'carpe diem', 'sorrow', 'january', 'funeral', 'travel', 'spring', 'fire', 'poem', 'suicide', 'remember', 'poetry', 'football', 'green', 'laughter', 'cinderella', 'poverty', 'today', 'birth', 'happiness', 'life', 'son', 'winter', 'hair', 'rose', 'rainbow', 'god', 'star', 'house', 'power', 'sympathy', 'depression', 'snake', 'night', 'sick', 'despair', 'husband', 'rain', 'love', 'river', 'city', 'childhood', 'sister', 'war', 'kiss', 'beach', 'marriage', 'sometimes', 'father', 'joy', 'success', 'hero', 'respect', 'mirror', 'justice', 'lonely', 'dream', 'song', 'hunting', 'funny', 'pink', 'anger', 'hope', 'friend', 'beautiful', 'america', 'death', 'wedding', 'alone', 'graduation', 'baby', 'water', 'fear', 'moon', 'beauty', 'sky', 'ocean', 'family', 'thanks', 'swimming', 'summer', 'freedom', 'dark', 'hate', 'music', 'warning', 'teacher', 'peace', 'chicago', 'butterfly', 'murder', '__index_level_0__'],\n",
       "        num_rows: 10988\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'passion', 'daughter', 'weather', 'animal', 'sea', 'innocence', 'change', 'angel', 'brother', 'world', 'paris', 'work', 'soldier', 'together', 'christmas', 'dance', 'sleep', 'happy', 'faith', 'school', 'courage', 'loss', 'romance', 'nature', 'identity', 'car', 'greed', 'lost', 'june', 'truth', 'destiny', 'trust', 'children', 'believe', 'culture', 'silver', 'frog', 'red', 'food', 'sun', 'romantic', 'time', 'lust', 'mother', 'heaven', 'memory', 'crazy', 'money', 'racism', 'girl', 'home', 'evil', 'future', 'running', 'carpe diem', 'sorrow', 'january', 'funeral', 'travel', 'spring', 'fire', 'poem', 'suicide', 'remember', 'poetry', 'football', 'green', 'laughter', 'cinderella', 'poverty', 'today', 'birth', 'happiness', 'life', 'son', 'winter', 'hair', 'rose', 'rainbow', 'god', 'star', 'house', 'power', 'sympathy', 'depression', 'snake', 'night', 'sick', 'despair', 'husband', 'rain', 'love', 'river', 'city', 'childhood', 'sister', 'war', 'kiss', 'beach', 'marriage', 'sometimes', 'father', 'joy', 'success', 'hero', 'respect', 'mirror', 'justice', 'lonely', 'dream', 'song', 'hunting', 'funny', 'pink', 'anger', 'hope', 'friend', 'beautiful', 'america', 'death', 'wedding', 'alone', 'graduation', 'baby', 'water', 'fear', 'moon', 'beauty', 'sky', 'ocean', 'family', 'thanks', 'swimming', 'summer', 'freedom', 'dark', 'hate', 'music', 'warning', 'teacher', 'peace', 'chicago', 'butterfly', 'murder', '__index_level_0__'],\n",
       "        num_rows: 2748\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = datasets.DatasetDict({\"train\":Dataset.from_pandas(train_dataset),\"test\":Dataset.from_pandas(test_dataset)})\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "encoded_dataset = dd.map(preprocess_data, batched=True, remove_columns=dd['train'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "example = encoded_dataset['train'][1]\n",
    "# example = dd['train'][1]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  1045,\n",
       "  2156,\n",
       "  1996,\n",
       "  3548,\n",
       "  10998,\n",
       "  2006,\n",
       "  4823,\n",
       "  1037,\n",
       "  3147,\n",
       "  2162,\n",
       "  2299,\n",
       "  2006,\n",
       "  1996,\n",
       "  2346,\n",
       "  2027,\n",
       "  2907,\n",
       "  3031,\n",
       "  2037,\n",
       "  4409,\n",
       "  2004,\n",
       "  1996,\n",
       "  3554,\n",
       "  3632,\n",
       "  2006,\n",
       "  2035,\n",
       "  2154,\n",
       "  2146,\n",
       "  2954,\n",
       "  1010,\n",
       "  2233,\n",
       "  1010,\n",
       "  1998,\n",
       "  2954,\n",
       "  5607,\n",
       "  1010,\n",
       "  6614,\n",
       "  1998,\n",
       "  5607,\n",
       "  2157,\n",
       "  3251,\n",
       "  1996,\n",
       "  3103,\n",
       "  12342,\n",
       "  2015,\n",
       "  1010,\n",
       "  2023,\n",
       "  2003,\n",
       "  2162,\n",
       "  2023,\n",
       "  2003,\n",
       "  2162,\n",
       "  3251,\n",
       "  1996,\n",
       "  15811,\n",
       "  10364,\n",
       "  2015,\n",
       "  2023,\n",
       "  2003,\n",
       "  2162,\n",
       "  2073,\n",
       "  2057,\n",
       "  3102,\n",
       "  2256,\n",
       "  2814,\n",
       "  1010,\n",
       "  5208,\n",
       "  1998,\n",
       "  3428,\n",
       "  2005,\n",
       "  2027,\n",
       "  2024,\n",
       "  2256,\n",
       "  6716,\n",
       "  2005,\n",
       "  2613,\n",
       "  2065,\n",
       "  2027,\n",
       "  5454,\n",
       "  2114,\n",
       "  2149,\n",
       "  2005,\n",
       "  2178,\n",
       "  2027,\n",
       "  2175,\n",
       "  2000,\n",
       "  2162,\n",
       "  2138,\n",
       "  1997,\n",
       "  1037,\n",
       "  9210,\n",
       "  1997,\n",
       "  2576,\n",
       "  2273,\n",
       "  2040,\n",
       "  3233,\n",
       "  4142,\n",
       "  1025,\n",
       "  1999,\n",
       "  14908,\n",
       "  4822,\n",
       "  2145,\n",
       "  3173,\n",
       "  2037,\n",
       "  25636,\n",
       "  1996,\n",
       "  3548,\n",
       "  2954,\n",
       "  2127,\n",
       "  2053,\n",
       "  2028,\n",
       "  4832,\n",
       "  2039,\n",
       "  4902,\n",
       "  999,\n",
       "  2954,\n",
       "  1010,\n",
       "  2233,\n",
       "  1010,\n",
       "  1998,\n",
       "  2954,\n",
       "  5607,\n",
       "  1010,\n",
       "  6614,\n",
       "  1998,\n",
       "  5607,\n",
       "  2157,\n",
       "  3251,\n",
       "  1996,\n",
       "  3103,\n",
       "  12342,\n",
       "  2015,\n",
       "  1010,\n",
       "  2023,\n",
       "  2003,\n",
       "  2162,\n",
       "  2023,\n",
       "  2003,\n",
       "  2162,\n",
       "  3251,\n",
       "  1996,\n",
       "  15811,\n",
       "  10364,\n",
       "  2015,\n",
       "  7479,\n",
       "  1012,\n",
       "  13378,\n",
       "  5428,\n",
       "  4305,\n",
       "  1012,\n",
       "  4012,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0]}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] i see the soldiers marching on singing a cold war song on the road they hold onto their guns as the fighting goes on all day long fight, march, and fight shoot, aim and shoot right whether the sun shines, this is war this is war whether the rains pours this is war where we kill our friends, sisters and brothers for they are our enemies for real if they choose against us for another they go to war because of a handful of political men who stand alive ; in erect offices still holding their pens the soldiers fight until no one stands up anymore! fight, march, and fight shoot, aim and shoot right whether the sun shines, this is war this is war whether the rains pours www. sylviachidi. com [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['war']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменение формата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model!\n",
    "\n",
    "We are going to train the model using HuggingFace's Trainer API. This requires us to define 2 things: \n",
    "\n",
    "* `TrainingArguments`, which specify training hyperparameters. All options can be found in the [docs](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments). Below, we for example specify that we want to evaluate after every epoch of training, we would like to save the model every epoch, we set the learning rate, the batch size to use for training/evaluation, how many epochs to train for, and so on.\n",
    "* a `Trainer` object (docs can be found [here](https://huggingface.co/transformers/main_classes/trainer.html#id1))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метрики для оценивания будем использовать F1 score, так как ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8 # размер батча"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-sem_eval-english\", # папка для сохранения контрольных точек\n",
    "    evaluation_strategy = \"epoch\", # оценивание во время обучения\n",
    "    save_strategy = \"epoch\", # сохранение модели (после эпохи)\n",
    "    learning_rate=2e-5, # lr для AdamW оптимизатора\n",
    "    per_device_train_batch_size=batch_size, # размер батча при тренирвке\n",
    "    per_device_eval_batch_size=batch_size, # размер батча при оценивании\n",
    "    num_train_epochs=5, # число эпох обучения\n",
    "    weight_decay=0.01, # вес для слоёв\n",
    "    load_best_model_at_end=True, # загружать ли лучшую модель\n",
    "    metric_for_best_model=\"f1\", # метрика, на основе которой выберется лучшая модель\n",
    "    dataloader_drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to compute metrics while training. For this, we need to define a compute_metrics function, that returns a dictionary with the desired metric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "    \n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions)) # переводим вероятности в область от 0 до 1\n",
    "\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1 # если вероятность > treshold, то ставим 1 в предсказании (остальное 0)\n",
    "\n",
    "    y_true = labels # реальное значение топиков\n",
    "    \n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy,\n",
    "               'precision': precision,\n",
    "               'recall': recall\n",
    "              }\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    print(\"p.predictions:\", p.predictions)\n",
    "    print(\"preds:\", preds)\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoded_dataset['train'][0]['labels'].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1045,  2156,  1996,  3548, 10998,  2006,  4823,  1037,  3147,\n",
       "         2162,  2299,  2006,  1996,  2346,  2027,  2907,  3031,  2037,  4409,\n",
       "         2004,  1996,  3554,  3632,  2006,  2035,  2154,  2146,  2954,  1010,\n",
       "         2233,  1010,  1998,  2954,  5607,  1010,  6614,  1998,  5607,  2157,\n",
       "         3251,  1996,  3103, 12342,  2015,  1010,  2023,  2003,  2162,  2023,\n",
       "         2003,  2162,  3251,  1996, 15811, 10364,  2015,  2023,  2003,  2162,\n",
       "         2073,  2057,  3102,  2256,  2814,  1010,  5208,  1998,  3428,  2005,\n",
       "         2027,  2024,  2256,  6716,  2005,  2613,  2065,  2027,  5454,  2114,\n",
       "         2149,  2005,  2178,  2027,  2175,  2000,  2162,  2138,  1997,  1037,\n",
       "         9210,  1997,  2576,  2273,  2040,  3233,  4142,  1025,  1999, 14908,\n",
       "         4822,  2145,  3173,  2037, 25636,  1996,  3548,  2954,  2127,  2053,\n",
       "         2028,  4832,  2039,  4902,   999,  2954,  1010,  2233,  1010,  1998,\n",
       "         2954,  5607,  1010,  6614,  1998,  5607,  2157,  3251,  1996,  3103,\n",
       "        12342,  2015,  1010,  2023,  2003,  2162,  2023,  2003,  2162,  3251,\n",
       "         1996, 15811, 10364,  2015,  7479,  1012, 13378,  5428,  4305,  1012,\n",
       "         4012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train']['input_ids'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6976, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[ 0.1693, -0.0342,  0.2474, -0.0891,  0.1187,  0.0364,  0.2213, -0.1987,\n",
       "         -0.4812, -0.2975,  0.1478, -0.3266, -0.0094,  0.1391, -0.0206, -0.0684,\n",
       "          0.5259, -0.1974,  0.0869, -0.3148,  0.3708,  0.0456,  0.2298,  0.1605,\n",
       "          0.1040, -0.6016, -0.1807, -0.2613, -0.3627,  0.6999,  0.2443, -0.2586,\n",
       "          0.1490, -0.1866,  0.2348, -0.6191, -0.1054,  0.3113,  0.1990,  0.1593,\n",
       "         -0.1383, -0.2160, -0.0870, -0.3349,  0.3600, -0.2608,  0.2605, -0.5622,\n",
       "          0.2469, -0.2694,  0.0480,  0.2376, -0.0070, -0.3804,  0.0539,  0.1944,\n",
       "          0.0988,  0.0907, -0.0454, -0.3037,  0.3083, -0.0190, -0.0896, -0.1437,\n",
       "          0.3018, -0.3925,  0.1806, -0.1201,  0.1375, -0.3407,  0.3874,  0.1513,\n",
       "         -0.0252,  0.0338, -0.4169,  0.6547,  0.0755,  0.2609, -0.5953, -0.4057,\n",
       "          0.1371, -0.3547, -0.2773,  0.0252, -0.3120, -0.0121, -0.1351,  0.0286,\n",
       "          0.3554,  0.3274, -0.4795,  0.0557, -0.1353,  0.2002,  0.0945, -0.3551,\n",
       "          0.0752, -0.1148,  0.0613, -0.3209,  0.2787, -0.2766, -0.1213,  0.3373,\n",
       "         -0.3618, -0.1003, -0.4410, -0.0386,  0.1908, -0.0110,  0.4004,  0.2239,\n",
       "         -0.0114, -0.2903,  0.2801,  0.0876,  0.0106,  0.2364, -0.3576, -0.1399,\n",
       "         -0.0665,  0.4193,  0.1313, -0.1847, -0.1891, -0.0640, -0.0767,  0.3746,\n",
       "          0.3890,  0.4491,  0.0732,  0.2632,  0.1474,  0.0954, -0.4102,  0.0898,\n",
       "          0.4492,  0.3701, -0.7699,  0.2325,  0.0680, -0.5516, -0.1120, -0.3372]],\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forward pass\n",
    "outputs = model(input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0), labels=encoded_dataset['train'][0]['labels'].unsqueeze(0))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model, # модель для обучения\n",
    "    args, # аргументы для обучения\n",
    "    train_dataset=encoded_dataset[\"train\"], # датасет, используемый при обучении\n",
    "    eval_dataset=encoded_dataset[\"test\"], # датасет, используемый для оценивания\n",
    "#     tokenizer=tokenizer, # токенизатор\n",
    "    compute_metrics=compute_metrics # функция для подсчёта метрик при оценивании\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='6865' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  31/6865 00:36 < 2:21:32, 0.80 it/s, Epoch 0.02/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [191]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1630\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1631\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1632\u001b[0m )\n\u001b[1;32m-> 1633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1638\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\trainer.py:1902\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1900\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1902\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1905\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1906\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1907\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1908\u001b[0m ):\n\u001b[0;32m   1909\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1910\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\trainer.py:2638\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2620\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2621\u001b[0m \u001b[38;5;124;03mPerform a training step on a batch of inputs.\u001b[39;00m\n\u001b[0;32m   2622\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2635\u001b[0m \u001b[38;5;124;03m    `torch.Tensor`: The tensor with training loss on this batch.\u001b[39;00m\n\u001b[0;32m   2636\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2637\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m-> 2638\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   2641\u001b[0m     loss_mb \u001b[38;5;241m=\u001b[39m smp_forward_backward(model, inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\trainer.py:2583\u001b[0m, in \u001b[0;36mTrainer._prepare_inputs\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_inputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Union[torch\u001b[38;5;241m.\u001b[39mTensor, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Union[torch\u001b[38;5;241m.\u001b[39mTensor, Any]]:\n\u001b[0;32m   2579\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2580\u001b[0m \u001b[38;5;124;03m    Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and\u001b[39;00m\n\u001b[0;32m   2581\u001b[0m \u001b[38;5;124;03m    handling potential state.\u001b[39;00m\n\u001b[0;32m   2582\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2583\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2584\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2585\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe batch received was empty, your model won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be able to train on it. Double-check that your \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2587\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining dataset contains keys expected by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signature_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2588\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\trainer.py:2565\u001b[0m, in \u001b[0;36mTrainer._prepare_input\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   2561\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2562\u001b[0m \u001b[38;5;124;03mPrepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors.\u001b[39;00m\n\u001b[0;32m   2563\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Mapping):\n\u001b[1;32m-> 2565\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)({k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\trainer.py:2565\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2561\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2562\u001b[0m \u001b[38;5;124;03mPrepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors.\u001b[39;00m\n\u001b[0;32m   2563\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Mapping):\n\u001b[1;32m-> 2565\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)({k: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\trainer.py:2575\u001b[0m, in \u001b[0;36mTrainer._prepare_input\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   2570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mis_floating_point(data) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(data)):\n\u001b[0;32m   2571\u001b[0m         \u001b[38;5;66;03m# NLP models inputs are int/uint and those get adjusted to the right dtype of the\u001b[39;00m\n\u001b[0;32m   2572\u001b[0m         \u001b[38;5;66;03m# embedding. Other models such as wav2vec2's inputs are already float and thus\u001b[39;00m\n\u001b[0;32m   2573\u001b[0m         \u001b[38;5;66;03m# may need special handling to match the dtypes of the model\u001b[39;00m\n\u001b[0;32m   2574\u001b[0m         kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhf_deepspeed_config\u001b[38;5;241m.\u001b[39mdtype()})\n\u001b[1;32m-> 2575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценивание (вывод метрик)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='344' max='344' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [344/344 01:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.04290616884827614,\n",
       " 'eval_f1': 0.0,\n",
       " 'eval_roc_auc': 0.5,\n",
       " 'eval_accuracy': 0.0,\n",
       " 'eval_runtime': 60.6555,\n",
       " 'eval_samples_per_second': 45.305,\n",
       " 'eval_steps_per_second': 5.671,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка работоспособности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I'm happy I can finally train a model for multi-label classification\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "\n",
    "outputs = trainer.model(**encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logits that come out of the model are of shape (batch_size, num_labels). As we are only forwarding a single sentence through the model, the batch_size equals 1. The logits is a tensor that contains the (unnormalized) scores for every individual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 144])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To turn them into actual predicted labels, we first apply a sigmoid function independently to every score, such that every score is turned into a number between 0 and 1, that can be interpreted as a \"probability\" for how certain the model is that a given class belongs to the input text.\n",
    "\n",
    "Next, we use a threshold (typically, 0.5) to turn every probability into either a 1 (which means, we predict the label for the given example) or a 0 (which means, we don't predict the label for the given example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0072, 0.0081, 0.0105, 0.0096, 0.0081, 0.0085, 0.0091, 0.0079, 0.0085,\n",
      "        0.0090, 0.0093, 0.0089, 0.0073, 0.0088, 0.0096, 0.0092, 0.0084, 0.0104,\n",
      "        0.0075, 0.0085, 0.0075, 0.0081, 0.0092, 0.0089, 0.0085, 0.0082, 0.0088,\n",
      "        0.0083, 0.0093, 0.0097, 0.0082, 0.0086, 0.0096, 0.0082, 0.0085, 0.0077,\n",
      "        0.0084, 0.0082, 0.0083, 0.0105, 0.0072, 0.0080, 0.0092, 0.0075, 0.0096,\n",
      "        0.0102, 0.0089, 0.0092, 0.0074, 0.0081, 0.0089, 0.0088, 0.0075, 0.0089,\n",
      "        0.0091, 0.0093, 0.0086, 0.0085, 0.0082, 0.0072, 0.0085, 0.0083, 0.0093,\n",
      "        0.0081, 0.0069, 0.0087, 0.0082, 0.0074, 0.0076, 0.0101, 0.0083, 0.0080,\n",
      "        0.0079, 0.0097, 0.0093, 0.0091, 0.0085, 0.0098, 0.0084, 0.0078, 0.0090,\n",
      "        0.0078, 0.0078, 0.0091, 0.0099, 0.0066, 0.0086, 0.0091, 0.0087, 0.0088,\n",
      "        0.0084, 0.0073, 0.0085, 0.0090, 0.0087, 0.0087, 0.0083, 0.0086, 0.0097,\n",
      "        0.0061, 0.0079, 0.0087, 0.0076, 0.0083, 0.0081, 0.0083, 0.0089, 0.0098,\n",
      "        0.0072, 0.0071, 0.0098, 0.0084, 0.0073, 0.0073, 0.0079, 0.0104, 0.0081,\n",
      "        0.0091, 0.0090, 0.0089, 0.0083, 0.0104, 0.0093, 0.0112, 0.0080, 0.0087,\n",
      "        0.0072, 0.0071, 0.0079, 0.0081, 0.0080, 0.0071, 0.0074, 0.0082, 0.0079,\n",
      "        0.0079, 0.0080, 0.0089, 0.0098, 0.0118, 0.0099, 0.0089, 0.0080, 0.0084],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# apply sigmoid + threshold\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "print(probs)\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
